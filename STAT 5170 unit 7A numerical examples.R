# =================================================================================
# STAT 5170: Applied Time Series
# Numerical examples for part A of learning unit 7
# =================================================================================
#
# =================================================================================
# Cardiovascular Mortality in LA
# =================================================================================
#
# We previously encountered the cardiovascular mortality data in learning unit 5A, in our exploration of seasonal variables. These data are included in "astsa" package.

data(cmort, package = "astsa")

# The data set includes about ten years of weekly measurements, or about 500 data values.

n <- length(cmort)
n

# A plot of the time series is generated by the following code.

ts.plot(cmort, xlab="time", ylab="mortality")

# ---------------------------------------------------------------------------------
# Moving-average smoothing
# ---------------------------------------------------------------------------------

# The code below defines a function that implements moving-average smoothing. Tuning parameters are inferred from the argument, wgts, which is assumed to have an odd number of positive entries that sum to one.

ma.smooth <- function(x.ts, wgts) {
	n <- length(x.ts)
	mu.hat <- mean(x.ts)
	n.lag <- length(wgts)
	m <- (n.lag - 1) / 2
	lag.seq <- -m:m
	val.seq <- numeric(length=length(lag.seq))
	x.hat.ts <- x.ts
	for (t in 1:n) {
		idx.seq <- t-lag.seq
		sub.idx.in <- which((idx.seq >= 1) & (idx.seq <= n))
		val.seq[sub.idx.in] <- x.ts[idx.seq[sub.idx.in]]
		sub.idx.out <- which((idx.seq < 1) | (idx.seq > n))
		val.seq[sub.idx.out] <- mu.hat
		x.hat.ts[t] <- sum(wgts*val.seq)
	}
	return(x.hat.ts)
}

# For example, a smoothed time series calculated with uniform weights, and m set to 12 so that the length of the smoothing window is 25, which covers about 5% of the data.

m <- 12
n.lag <- 2*m+1
wgts <- rep(x=1, times=n.lag)/n.lag
x.hat.ts <- ma.smooth(cmort, wgts)

# The following code produces a graph of the cardiovascular mortality time series overlaid with several smoothed versions of that time series. The graph colored blue is calculated with m=25, hence a smoothing-window coverage of about 10%; that colored red has m=12 and 5% smoothing-window coverage; and that colored green has m=5 and 2% smoothing-window coverage  

m.seq <- c(25, 12, 5)
col.seq <- c("green", "red", "blue")
time.seq <- as.numeric(time(cmort))
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality", main="moving-average smoothing")
for (i.val in 1:length(m.seq)) {
	m <- m.seq[i.val]
	n.lag <- 2*m+1
	wgts <- rep(x=1, times=n.lag)/n.lag
	x.hat.ts <- ma.smooth(cmort, wgts)
	lines(time.seq, x.hat.ts, lty=1, lwd=3, col=col.seq[i.val])
}

# Observe that using the widest smoothing window "smooths out" the seasonal cycles. Using the narrowest smoothing window preserves many details of the seasonal cycles.

# ---------------------------------------------------------------------------------
# Kernel smoothing
# ---------------------------------------------------------------------------------

# In the time-domain perspective, when moving average smoothing is implemented using non-uniform weights is it typically called kernel smoothing.

# The following user-defined function translates the parameter m, which defines the smoothing window, to weight-values calculated from a Gaussian kernel such that 0.025 and 0.975 quantiles of the underying standard-normal density that defines it coincide with -m and m. 

Gauss.wgts <- function(m, alpha=0.05) {
	n.lag <- 2*m+1
	wgts <- numeric(length=n.lag)
	zval <- qnorm(p=1-alpha/2, mean=0, sd=1)
	sig <- m/zval
	for (i.lag in 1:n.lag) {
		val <- i.lag - (m + 1)
		wgts[i.lag] <- dnorm(x=val, mean=0, sd=sig)
	}
	wgts <- wgts / sum(wgts)
	return(wgts)
}

# A plot of the kernel function at a specified value m is generated as follows

m <- 12
wgts <- Gauss.wgts(m)
plot(-m:m, wgts, type="l", lty=1, lwd=3, col="blue", xlab="", ylab="", main="Gaussian kernel")
abline(h=0, lty=2, lwd=1)

# The previous graph of smoothed fitted values is reproduced by the following code, having substituted Gaussian-weighted smoothing for unweighted smoothing.

m.seq <- c(25, 12, 5)
col.seq <- c("green", "red", "blue")
time.seq <- as.numeric(time(cmort))
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality", main="kernel smoothing")
for (i.val in 1:length(m.seq)) {
	m <- m.seq[i.val]
	wgts <- Gauss.wgts(m)
	x.hat.ts <- ma.smooth(cmort, wgts)
	lines(time.seq, x.hat.ts, lty=1, lwd=3, col=col.seq[i.val])
}

# The following code generates output of the both plots presented in the same figure for comparison.

m.seq <- c(25, 12, 5)
col.seq <- c("green", "red", "blue")
time.seq <- as.numeric(time(cmort))
par(mfrow = c(2, 1))
par(mar=c(2.00, 2.00, 1.00, 1.00)) #bottom, left, top, and right
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality", main="moving average")
for (i.val in 1:length(m.seq)) {
	m <- m.seq[i.val]
	n.lag <- 2*m+1
	wgts <- rep(x=1, times=n.lag)/n.lag
	x.hat.ts <- ma.smooth(cmort, wgts)
	lines(time.seq, x.hat.ts, lty=1, lwd=3, col=col.seq[i.val])
}
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality", main="kernel smoothing")
for (i.val in 1:length(m.seq)) {
	m <- m.seq[i.val]
	wgts <- Gauss.wgts(m)
	x.hat.ts <- ma.smooth(cmort, wgts)
	lines(time.seq, x.hat.ts, lty=1, lwd=3, col=col.seq[i.val])
}
par(mfrow = c(1, 1))

# As we've seen perviously, in our exploration of smoothing the periodogram, weighted smoothing is better able to capture spikey patterns in the data.

# ---------------------------------------------------------------------------------
# Regression smoothing
# ---------------------------------------------------------------------------------

# Here we will use the user-defined function "reg.summary" to implement the regression calculations. This is a simplified version of the function of the same name from previous learning units.

reg.summary <- function(x.vect, Z.mat) {
	n <- dim(Z.mat)[1]
	r <- dim(Z.mat)[2]
	ZpZ <- t(Z.mat) %*% Z.mat
	ZpX <- t(Z.mat) %*% x.vect
	ZpZ.inv <- solve(ZpZ)
	b.hat <- ZpZ.inv %*% ZpX
	mu <- Z.mat %*% b.hat
	result <- list(b.hat=b.hat, mu=mu)
	return(result)
}

# The following code de-trends the time series using a linear formula for the mean function term plus two trigonometric terms that model a yearly cycle. To avoid neumerical problems, the time values are shifted and scaled to count number of weeks.

n <- length(cmort)
r <- 4
time.seq <- 1:n
omega <- 1/52
x.vect <- as.matrix(cmort)
Z.mat <- matrix(data=0, nrow=n, ncol=r)
Z.mat[,1] <- rep(x=1, times=n)
Z.mat[,2] <- time.seq
Z.mat[,3] <- cos(2*pi*omega*time.seq)
Z.mat[,4] <- sin(2*pi*omega*time.seq)

# The following code generates a graph of the time series overlaid with fitted values from two versions to the regression analysis. The graph colored blue is from an analysis that includes both the linear and seasonal varaibles. The graph colored green reflects an analysis with just the linear term.

plot(time.seq, as.numeric(x.vect), type="p", pch=16, cex=0.5, xlab="weeks", ylab="mortality", main="regression")
result <- reg.summary(x.vect, Z.mat)
lines(time.seq, result$mu, lty=1, lwd=3, col="blue")
result <- reg.summary(x.vect, Z.mat[,1:2])
lines(time.seq, result$mu, lty=1, lwd=3, col="green")

# ---------------------------------------------------------------------------------
# Local regression with cubic B-splines
# ---------------------------------------------------------------------------------

# An individual cubic B-spline function is defined in separate sections that "knot" together along a sequence of evenly-spaced values. The function is located around a central knot, and its width is determined by the distance between adjacent knots.

# The following user-defined function calculates a cubic B-spline function defined by the location of its central knot and the between-knot distance.

cubic.B.spline <- function(x, cknot, kdist) {
	sect <- floor((x-cknot)/kdist)
	lknot <- cknot + kdist*sect
	u <- (x-lknot)/kdist
	val <- 0
	if (sect == -2) {
		val <- u^3/6
	} else if (sect == -1) {
		val <- (1 + 3*u + 3*u^2 - 3*u^3)/6
	} else if (sect == 0) {
		val <- (4 - 6*u^2 + 3*u^3)/6
	} else if (sect == 1) {
		val <- (1 - 3*u + 3*u^2 - u^3)/6
	}
	return(val)
}

# A plot of this function is generated by the code below

cknot <- 4
kdist <- 2
n.pts <- 50
grid.pts <- seq(from=cknot-2.5*kdist, to=cknot+2.5*kdist, length.out=n.pts)
cBs.vals <- numeric(length=n.pts)
for (i.pts in 1:n.pts) {
	cBs.vals[i.pts] <- cubic.B.spline(grid.pts[i.pts], cknot, kdist)
}
plot(grid.pts, cBs.vals, type="l", lty=1, lwd=3, col="blue", xlab="", ylab="", main="cubic B-spline")
abline(h=0, lty=2, lwd=1)

# In non-parametric regression, cubic B-splines are used to construct regressor variables. Each variable is defined by a cubic B-spline at a particular central-knot location from a grid of evenly spaced central-knot locations. The following code generates a plot of multiple cubic B-spline, overlaid in the same plot. The functions are evenly spaced at about 46-week distances between central knots, and about 31-week distances between the knots that define an individual spline.

nspline <- 11
ratio <- 1.5
ckdist <- (n-1) / nspline
kdist <- ckdist/ratio
st.cknot <- 1+0.5*ckdist
cknot.grid <- seq(from=st.cknot, to=st.cknot+ckdist*nspline, by=ckdist)
plot(c(1,n), c(0,0), type="l", lty=1, lwd=3, col="white", xlim=c(0,n+1), ylim=c(0,0.65), xlab="", ylab="", main="cubic B-splines")
abline(h=0, lty=2, lwd=1)
n.pts <- 50
spl.vals <- numeric(length=n.pts)
for (i.spl in 1:nspline) {
	cknot <- cknot.grid[i.spl]
	grid.pts <- seq(from=cknot-2.5*kdist, to=cknot+2.5*kdist, length.out=n.pts)
	for (i.pts in 1:n.pts) {
		spl.vals[i.pts] <- cubic.B.spline(grid.pts[i.pts], cknot, kdist)
	}
	lines(grid.pts, spl.vals, type="l", lty=1, lwd=3, col="blue")
}

# The following user-define function calculates a regression matrix from these functions. The matrix includes one column for the intercept term, and one additional column for each cubic B-spline function. Input arguments are the number of time points, n, the number of cubic B-spline functions that are to cover the time period from 1 to n, and a parameter called "ratio," which specifies the ratio of the distance between central knots to the between-knot distances that define an individual spline. The larger this ratio, the thinner is each indidivual spline.

cbspline.regmat <- function(n, nspline, ratio) {
	ckdist <- (n-1) / nspline
	kdist <- ckdist/ratio
	st.cknot <- 1+0.5*ckdist
	Z.mat <- matrix(data=0, nrow=n, ncol=nspline+1)
	Z.mat[,1] <- rep(x=1, times=n)
	cknot.grid <- seq(from=st.cknot, to=st.cknot+ckdist*nspline, by=ckdist)
	for (i.spl in 1:nspline) {
		n.col <- i.spl + 1
		cknot <- cknot.grid[i.spl]
		for (t in 1:n) {
			Z.mat[t, n.col] <- cubic.B.spline(t, cknot, kdist)
		}
	}
	return(Z.mat)
}

# The regression problem is set up as follows.

time.seq <- 1:n
x.vect <- as.matrix(cmort)
Z.mat <- cbspline.regmat(n, nspline=11, ratio=1.5)

# A plot of the time series overlaid with the fitted values from the regression calculation is generated as follows.

plot(time.seq, as.numeric(x.vect), type="p", pch=16, cex=0.5, xlab="weeks", ylab="mortality", main="cubic B-spline regression")
result <- reg.summary(x.vect, Z.mat)
lines(time.seq, result$mu, lty=1, lwd=3, col="blue")

# In an effort to capture the seasonal patterns, we could modify the analysis by adding more cubic B-splines, and possibly changing the ratio parameter. However, it is perhaps simpler to just add trigonometric terms to the regression matrix.

omega <- 1/52
r <- dim(Z.mat)[2]
Z.mat <- cbind(Z.mat, matrix(data=0, nrow=n, ncol=2))
Z.mat[,r+1] <- cos(2*pi*omega*time.seq)
Z.mat[,r+2] <- sin(2*pi*omega*time.seq)
r <- r + 2

# A plot of the resulting fitted values is generated as follows. 

plot(time.seq, as.numeric(x.vect), type="p", pch=16, cex=0.5, xlab="weeks", ylab="mortality", main="cubic B-spline regression")
result <- reg.summary(x.vect, Z.mat[,1:(r-2)])
lines(time.seq, result$mu, lty=1, lwd=3, col="green")
result <- reg.summary(x.vect, Z.mat)
lines(time.seq, result$mu, lty=1, lwd=3, col="blue")

# ---------------------------------------------------------------------------------
# Local polynomial regression
# ---------------------------------------------------------------------------------

# Regression smoothing works "locally" within the smoothing window to calculate a summary of values within that particular interval. Whereas moving-average smoothing defines this summary be averaging, regression smoothing defines it by fitting the center value of the smoothing window under a polynomial regression model.

# This is illustrated in the diagram produced by the following code.
 
t <- 40
m <- 5
n.lag <- 2*m + 1
lag.seq <- -m:m
time.seq <- t-lag.seq
val.seq <- cmort[time.seq]
x.vect <- as.matrix(val.seq)
Z.mat <- matrix(data=0, nrow=n.lag, ncol=3)
Z.mat[,1] <- rep(x=1, times=n.lag)
Z.mat[,2] <- time.seq
Z.mat[,3] <- time.seq^2
result <- reg.summary(x.vect, Z.mat)

disp.time.seq <- t-(-(m+2):(m+2))
disp.val.seq <- cmort[disp.time.seq]
ymin <- min(disp.val.seq, result$mu)
ymax <- max(disp.val.seq, result$mu)
yrng <- ymax-ymin
plot(disp.time.seq, disp.val.seq, type="p", pch=16, cex=1, xlim=disp.time.seq[c(length(disp.time.seq),1)], ylim=c(ymin-0.1*yrng, ymax+0.1*yrng), xlab="week", ylab="mortality")
lines(time.seq, result$mu, lty=1, lwd=1, col="blue")
points(time.seq[m+1], result$mu[m+1,1], pch=8, cex=1, col="blue")
text(x=time.seq[m+1], y=ymax-0.1*yrng, labels="smoothing window", pos=1)
abline(v=time.seq[c(1,n.lag)]+0.5*c(1,-1), lty=2, lwd=1, col="blue")

# Local smoothing can also be weighted by incorporating modified version of regression analysis that weights some data points more heavily than others, giving them more influence in the final calculation.

# The user-defined function created by the following code incorporates such weigthing into regression analysis. Notice the structural similarity of its calculation to a previous calculation that would accommodate autocorrelated residuals through the use of a correlation matrix.

reg.summary.wgt <- function(x.vect, Z.mat, wgts) {
	n <- dim(Z.mat)[1]
	r <- dim(Z.mat)[2]
	W.mat <- diag(wgts)
	ZpWZ <- as.matrix(t(Z.mat) %*% W.mat %*% Z.mat)
	ZpWX <- as.matrix(t(Z.mat) %*% W.mat %*% x.vect)
	ZpWZ.inv <- solve(ZpWZ)
	b.hat <- ZpWZ.inv %*% ZpWX
	mu <- Z.mat %*% b.hat
	result <- list(b.hat=b.hat, mu=mu)
	return(result)
}

# Weighted local regression is illustrated in the plot produced as follows.

wgts <- Gauss.wgts(m)
result <- reg.summary.wgt(x.vect, Z.mat, wgts)

disp.time.seq <- t-(-(m+2):(m+2))
disp.val.seq <- cmort[disp.time.seq]
disp.wgts <- 0.25 + 0.75*c(0, 0, wgts, 0, 0) / min(wgts)
ymin <- min(disp.val.seq, result$mu)
ymax <- max(disp.val.seq, result$mu)
yrng <- ymax-ymin
plot(disp.time.seq, disp.val.seq, type="p", pch=16, cex= disp.wgts, xlim=disp.time.seq[c(length(disp.time.seq),1)], ylim=c(ymin-0.1*yrng, ymax+0.1*yrng), xlab="week", ylab="mortality")
lines(time.seq, result$mu, lty=1, lwd=1, col="blue")
points(time.seq[m+1], result$mu[m+1,1], pch=8, cex=1, col="blue")
text(x=time.seq[m+1], y=ymax-0.1*yrng, labels="smoothing window", pos=1)
abline(v=time.seq[c(1,n.lag)]+0.5*c(1,-1), lty=2, lwd=1, col="blue")

# Local regression smoothing is implemented by the following user-defined function. The parameter is inferred from the length of the weight sequence, which is supplied as an input argument. An additional input parameter, d, is also to be supplied to indicate the order of polynomial regression, which default value is d=2 for quadratic regression.

reg.smooth <- function(x.ts, wgts, d=2) {
	n <- length(x.ts)
	mu.hat <- mean(x.ts)
	n.lag <- length(wgts)
	m <- 0.5*(n.lag-1)
	lag.seq <- -m:m
	val.seq <- numeric(length=length(lag.seq))
	n.col <- d+1
	Z.mat <- matrix(data=0, nrow=n.lag, ncol=n.col)
	Z.mat[,1] <- rep(x=1, times=n.lag)
	x.hat.ts <- x.ts
	for (t in 1:n) {
		idx.seq <- t-lag.seq
		sub.idx.in <- which((idx.seq >= 1) & (idx.seq <= n))
		val.seq[sub.idx.in] <- x.ts[idx.seq[sub.idx.in]]
		sub.idx.out <- which((idx.seq < 1) | (idx.seq > n))
		val.seq[sub.idx.out] <- mu.hat
		x.vect <- as.matrix(val.seq)
		for (i.col in 2:n.col) {
			Z.mat[, i.col] <- lag.seq^(i.col-1)
		}
		result <- reg.summary.wgt(x.vect, Z.mat, wgts)
		x.hat.ts[t] <- result$mu[m+1,1]
	}
	return(x.hat.ts)
}

# The following code reproduces a graph similar to those generated above. It displays the cardiovascular mortality time series overlaid with several smoothed versions of that time series calculated using regression smoothing. The graph colored blue is calculated with m=25, hence a smoothing-window coverage of about 10%; that colored red has m=12 and 5% smoothing-window coverage; and that colored green has m=5 and 2% smoothing-window coverage  

m.seq <- c(25, 12, 5)
col.seq <- c("green", "red", "blue")
time.seq <- as.numeric(time(cmort))
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality")
for (i.val in 1:length(m.seq)) {
	m <- m.seq[i.val]
	wgts <- Gauss.wgts(m)
	x.hat.ts <- reg.smooth(cmort, wgts)
	lines(time.seq, x.hat.ts, lty=1, lwd=3, col=col.seq[i.val])
}

# Local regression smoothing with m=12 offers a nice balance between under- and over-smoothing

m <- 12
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality")
wgts <- Gauss.wgts(m)
x.hat.ts <- reg.smooth(cmort, wgts)
lines(time.seq, x.hat.ts, lty=1, lwd=3, col="blue")

# An interesting property of local regression smoothing that its benefit is only realized when the order of the polynomial used in local fitting is at least quadratic. Compare, for instance, local linear regression smoothing (with d=1), and ordinary kernel smoothing, as the figure produced by the following code illustrates.

m.seq <- c(25, 12, 5)
col.seq <- c("green", "red", "blue")
time.seq <- as.numeric(time(cmort))
par(mfrow = c(2, 1))
par(mar=c(2.00, 2.00, 1.00, 1.00)) #bottom, left, top, and right
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality", main="kernel smoothing")
for (i.val in 1:length(m.seq)) {
	m <- m.seq[i.val]
	wgts <- Gauss.wgts(m)
	x.hat.ts <- ma.smooth(cmort, wgts)
	lines(time.seq, x.hat.ts, lty=1, lwd=3, col=col.seq[i.val])
}
ts.plot(cmort, gpars=list(type="p", pch=16, cex=0.5), xlab="time", ylab="mortality", main="regression smoothing, d=1")
for (i.val in 1:length(m.seq)) {
	m <- m.seq[i.val]
	wgts <- Gauss.wgts(m)
	x.hat.ts <- reg.smooth(cmort, wgts, d=1)
	lines(time.seq, x.hat.ts, lty=1, lwd=3, col=col.seq[i.val])
}
par(mfrow = c(1, 1))

# The two plots in the figure are exactly the same. This is the a reflection of the property that, in the linear case, a fitted regression line passes through the point (zbar, xbar).


